{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4b3887-22f4-4d49-8bd6-38a97784d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/don/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9156c6e-561f-4007-ae3f-a1fffb5aa2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'The first step to get a bag of words vector is to split the text into words (tokens) and then reduce words to their base forms. For example, “running” will transform into “run”. This process is called stemming. We can use the NLTK Python package for it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ce6fc5-1101-4f0a-b650-d5cf0682867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'first', 'step', 'to', 'get', 'a', 'bag', 'of', 'words', 'vector', 'is', 'to', 'split', 'the', 'text', 'into', 'words', '(', 'tokens', ')', 'and', 'then', 'reduce', 'words', 'to', 'their', 'base', 'forms', '.', 'For', 'example', ',', '“', 'running', '”', 'will', 'transform', 'into', '“', 'run', '”', '.', 'This', 'process', 'is', 'called', 'stemming', '.', 'We', 'can', 'use', 'the', 'NLTK', 'Python', 'package', 'for', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(quote)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093c43f2-ec9e-4fc0-85a2-00d794f4dd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(language = \"english\")\n",
    "stemmed_words = list(map(lambda x: stemmer.stem(x), tokens))\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = stemmer.stem(tokens[i])\n",
    "print(stemmed_words==stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479cf9d1-650d-43d0-acc7-30c38d8451c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'.': 4, 'the': 3, 'to': 3, 'word': 3, 'is': 2, 'into': 2, 'for': 2, '“': 2, 'run': 2, '”': 2, 'first': 1, 'step': 1, 'get': 1, 'a': 1, 'bag': 1, 'of': 1, 'vector': 1, 'split': 1, 'text': 1, '(': 1, 'token': 1, ')': 1, 'and': 1, 'then': 1, 'reduc': 1, 'their': 1, 'base': 1, 'form': 1, 'exampl': 1, ',': 1, 'will': 1, 'transform': 1, 'this': 1, 'process': 1, 'call': 1, 'stem': 1, 'we': 1, 'can': 1, 'use': 1, 'nltk': 1, 'python': 1, 'packag': 1, 'it': 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "bag_of_words = collections.Counter(stemmed_words)\n",
    "print(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7ab898-b0e8-4457-8e7e-947fb997773a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def createMap(collections) -> list[(str,int)]:\n",
    "    mapCount = {}\n",
    "    for item in collections:\n",
    "        if item in mapCount:\n",
    "            mapCount[item] += 1\n",
    "        else:\n",
    "            mapCount[item] = mapCount.get(item,0) + 1\n",
    "    return mapCount\n",
    "print(createMap(stemmed_words)==bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53077ce5-75fd-49a8-88c4-4eee5a3aba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'bob': 1}), Counter({'alic': 1}), Counter({'frank': 1})]\n"
     ]
    }
   ],
   "source": [
    "doc_1 = \"Bob\"\n",
    "doc_2 = \"Alice\"\n",
    "doc_3 = \"Frank\"\n",
    "docs_collection = [doc_1, doc_2, doc_3]\n",
    "\n",
    "bow_collections = []\n",
    "stemmed_collections = []\n",
    "\n",
    "for doc in docs_collection:\n",
    "    tokens = word_tokenize(doc)\n",
    "    stemmed_tokens = list(map(lambda x: stemmer.stem(x), tokens))\n",
    "    stemmed_collections.append(stemmed_tokens)\n",
    "\n",
    "bow_collections = [collections.Counter(doc) for doc in stemmed_collections]\n",
    "print(bow_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c742b923-f86e-40ac-bcf2-551f6eeb61a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bob': 0, 'alic': 0, 'frank': 0}\n"
     ]
    }
   ],
   "source": [
    "def createSegment(segTemp, newDic):\n",
    "    for key, _ in newDic.items():\n",
    "        segTemp[key] = segTemp.get(key,0)\n",
    "    return segTemp\n",
    "\n",
    "segTemp = {}\n",
    "for bow in bow_collections:\n",
    "    segTemp = createSegment(segTemp, bow)\n",
    "print(segTemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9200372f-3947-4be4-a1fb-4043d5d9df72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vec  0 :  {'bob': 1, 'alic': 0, 'frank': 0} \n",
      "\n",
      "Vec  1 :  {'bob': 0, 'alic': 1, 'frank': 0} \n",
      "\n",
      "Vec  2 :  {'bob': 0, 'alic': 0, 'frank': 1} \n",
      "\n",
      "[{'bob': 1, 'alic': 0, 'frank': 0}, {'bob': 0, 'alic': 1, 'frank': 0}, {'bob': 0, 'alic': 0, 'frank': 1}]\n"
     ]
    }
   ],
   "source": [
    "simple_vec = segTemp.copy()\n",
    "\n",
    "count = 0\n",
    "vec_collection = []\n",
    "for bow in bow_collections:\n",
    "    simple_vec = segTemp.copy()\n",
    "    for key, value in bow.items():\n",
    "        simple_vec[key] = value\n",
    "    print(\"Vec \" ,count, \": \", simple_vec, \"\\n\")\n",
    "    count +=1\n",
    "    vec_collection.append(simple_vec)\n",
    "print(vec_collection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78eab0e3-9f29-4358-ae43-247d391778b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tokenizeStemBow(docs_collection):\n",
    "    stemmed_collections = []\n",
    "    \n",
    "    for doc in docs_collection:\n",
    "        tokens = word_tokenize(doc)\n",
    "        stemmed_tokens = list(map(lambda x: stemmer.stem(x), tokens))\n",
    "        stemmed_collections.append(stemmed_tokens)\n",
    "    \n",
    "    bow_collections = [collections.Counter(doc) for doc in stemmed_collections]\n",
    "    return bow_collections\n",
    "\n",
    "def createSegment(segTemp, newDic):\n",
    "    for dic in newDic:\n",
    "        for key, _ in dic.items():\n",
    "            if key not in segTemp:\n",
    "                segTemp[key] = 0\n",
    "    return segTemp\n",
    "\n",
    "    \n",
    "def createEmbedding(bow_collection, collection):\n",
    "    vec_collection = []\n",
    "    for bow in bow_collection:\n",
    "        simple_vec = collection.copy()\n",
    "        for key, value in bow.items():\n",
    "            simple_vec[key] = value\n",
    "        vec_collection.append(simple_vec)\n",
    "    return vec_collection\n",
    "\n",
    "def computeTF(collection) -> list[dict[str,float]]:\n",
    "    col = [doc.copy() for doc in collection]\n",
    "    for dic in col:\n",
    "        n = 0\n",
    "        for key, value in dic.items():\n",
    "            n += value\n",
    "            \n",
    "        for key, value in dic.items():\n",
    "            if dic[key] > 0 and n > 0:\n",
    "                dic[key] = value / n\n",
    "    return col\n",
    "\n",
    "def computeIDF(collection) -> list[dict[str,float]]:\n",
    "    N = len(collection)\n",
    "    sample = collection[0].copy()\n",
    "    for key, _ in sample.items():\n",
    "        sample[key] = 0\n",
    "        \n",
    "    for key, _ in sample.items():\n",
    "        for i in range(N):\n",
    "            if collection[i][key] > 0:\n",
    "                sample[key] += 1\n",
    "            \n",
    "    for key, value in sample.items():\n",
    "        sample[key] = math.log(N / value)\n",
    "    return sample\n",
    "\n",
    "def computeTFIDF(collection) -> list[dict[str,float]]:\n",
    "    tf_list = computeTF(collection)\n",
    "    idf_list = computeIDF(collection)\n",
    "    for key, _ in collection[0].items():\n",
    "        for i in range(len(collection)):\n",
    "            collection[i][key]= tf_list[i][key] * idf_list[key]\n",
    "            #print(\"collection \", i, \", key \", key, \": \", tf_list[i][key], \" * \", idf_list[key], \" = \", collection[i][key])\n",
    "    #print(\"\\n\")\n",
    "    return collection\n",
    "\n",
    "def computeWeights(weight: str, collection):\n",
    "    if weight == \"tf\":\n",
    "        return computeTF(collection)\n",
    "    elif weight == \"idf\":\n",
    "        return computeIDF(collection)\n",
    "    elif weight == \"tfidf\":\n",
    "        return computeTFIDF(collection)\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8811762-ac6b-4d54-9c1e-33aa49944288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7890412047105388, 0.07192051811294521, 0.07192051811294521, 0.23104906018664842]\n"
     ]
    }
   ],
   "source": [
    "def tokenizeSegmentEmbedding(bow_collection):\n",
    "    collection = {}\n",
    "    bow_collection = tokenizeStemBow(bow_collection)\n",
    "    collection = createSegment({},bow_collection)\n",
    "    collection = createEmbedding(bow_collection,collection)\n",
    "    return collection\n",
    "\n",
    "def computeRelevance(rel: str, query: list[dict[str,int]], collection: list[dict[str, float]])-> list[dict[str,float]]:\n",
    "    if rel == \"sow\":\n",
    "        return sumOfWeights(query, collection)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "def sumOfWeights(query, collection):\n",
    "    relevance = []\n",
    "    for doc in collection:\n",
    "        rel_score = 0\n",
    "        for key, value in query[0].items():\n",
    "            rel_score += value * doc[key]\n",
    "        relevance.append(rel_score)\n",
    "    return relevance\n",
    "\n",
    "def computeScore(rel, query, collection):\n",
    "    queryBow = [query]\n",
    "    collection = queryBow + collection\n",
    "    queryBow = tokenizeSegmentEmbedding(queryBow)\n",
    "    collection = tokenizeSegmentEmbedding(collection)\n",
    "    collectionWeights = computeWeights(\"tfidf\", collection)\n",
    "    relevance_score = computeRelevance(rel, queryBow, collectionWeights)\n",
    "    return relevance_score\n",
    "\n",
    "relevance_score = computeScore(\"sow\", \"Who is batman\", [\"My name is Bob\", \"My name is Alice\", \"I am Batman\"])\n",
    "print(relevance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c9db34-ff8d-48fa-8ee8-cb16b9523b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance score for doc 1  :  0.0\n",
      "Relevance score for doc 2  :  0.0\n",
      "Relevance score for doc 3  :  0.08664339756999316\n"
     ]
    }
   ],
   "source": [
    "query = \"Who likes football?\"\n",
    "text1 = \"My name is alice and i like ping pong.\"\n",
    "text2 = \"My name is franc and i like swimming.\"\n",
    "text3 = \"My name is bob and i like football\"\n",
    "collection = [text1, text2, text3]\n",
    "relevance_score = computeScore(\"sow\", query, collection)\n",
    "for i in range(len(collection)):\n",
    "    print(\"Relevance score for doc\", i+1, \" : \", relevance_score[i+1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
