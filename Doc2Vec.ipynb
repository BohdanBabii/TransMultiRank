{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fd8dea6-89e8-4de0-9e3e-d2d69d500b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/don/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/don/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import ir_datasets\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "ds = ir_datasets.load(\"msmarco-passage-v2/trec-dl-2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e81106c-8dc5-4cb6-9ad0-a42b0b556a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs Count :  138364198\n",
      "Queries Count :  500\n",
      "Qrels Count :  386416\n"
     ]
    }
   ],
   "source": [
    "print(\"Docs Count : \", ds.docs_count())\n",
    "print(\"Queries Count : \", ds.queries_count())\n",
    "print(\"Qrels Count : \", ds.qrels_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96a5e198-0d73-47f2-8239-03746e84c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd3688a3-3173-426c-87c3-d71c9f1c44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_queries = {}\n",
    "for query in ds.queries_iter():\n",
    "    processed_queries[query.query_id] = preprocess(query.text)\n",
    "\n",
    "with open('queries.npy', 'wb+') as f:\n",
    "    np.save(f, processed_queries, allow_pickle=True)\n",
    "\n",
    "loaded = np.load('queries.npy', allow_pickle=True).item()\n",
    "assert loaded == processed_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb170244-bc94-4f1a-a45e-cc0ec9186f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCS = 250_000\n",
    "BATCH_SIZE = 50_000\n",
    "it = islice(ds.docs_iter(), MAX_DOCS)\n",
    "batch_idx = 0\n",
    "while True:\n",
    "    batch = list(islice(it, BATCH_SIZE))\n",
    "    if not batch:\n",
    "        break\n",
    "\n",
    "    processed = {doc.doc_id: preprocess(doc.text) for doc in batch}\n",
    "    np.save(f\"batches/docs_batch_{batch_idx:03d}.npy\", processed, allow_pickle=True)\n",
    "    batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd3a8737-4cef-4833-a597-0a9bd6bb27b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "def load_tagged_documents(batch_files):\n",
    "    for batch_file in batch_files:\n",
    "        data = np.load(batch_file, allow_pickle=True).item()\n",
    "        for doc_id, text in data.items():\n",
    "            tokens = text.split()\n",
    "            yield TaggedDocument(words=tokens, tags=[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7c279-10e7-41a3-ab13-54c118c09f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import glob\n",
    "\n",
    "vector_size = 512\n",
    "window = 5\n",
    "min_count = 2\n",
    "epochs = 20\n",
    "\n",
    "batch_files = sorted(glob.glob(\"batches/docs_batch_*.npy\"))\n",
    "tagged_docs = list(load_tagged_documents(batch_files))\n",
    "\n",
    "model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=4, epochs=epochs)\n",
    "model.build_vocab(tagged_docs)\n",
    "\n",
    "model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"doc2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acdf18-2466-4c45-87c3-38624b8828cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"doc2vec_model.model\")\n",
    "\n",
    "new_doc = \"Sample text for embedding.\"\n",
    "tokens = preprocess(new_doc).split()\n",
    "\n",
    "vector = model.infer_vector(tokens)\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f4e38-0695-4dd6-9d25-7233f01d8559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
